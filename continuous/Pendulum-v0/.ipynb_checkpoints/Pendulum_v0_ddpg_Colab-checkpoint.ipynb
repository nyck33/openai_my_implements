{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W0S4KBFHHOEN"
   },
   "source": [
    "# Pendulum-v0 DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tsjKxyGBcINS"
   },
   "source": [
    "Tinker with learning rates and noise (scaling formula), need more exploration as it seems stuck, mean not improving, just a few lucky hits but that stops as well.  \n",
    "Also want to decay the noise as it seems to be the reason why I am getting some freakish -1.3 super low scores.  \n",
    "And the mean looks stuck getting to about 700 and then back up to 1300 or 1400 and then cycle.  \n",
    "Now we have enough exploration and need to exploit more later.  Is this a DDPG trait?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "2-7l6CXmHOEP",
    "outputId": "3325f04c-1686-4db0-d927-e25e84f9b49a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.10.11)\n",
      "Requirement already satisfied: keras==2.1.6 in /usr/local/lib/python3.6/dist-packages (2.1.6)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.2.1)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.16.3)\n",
      "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (2.21.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.12.0)\n",
      "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.1.6) (3.13)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.1.6) (2.8.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (1.24.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2019.3.9)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2.8)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym) (0.16.0)\n"
     ]
    }
   ],
   "source": [
    "#from my_ddpg_pendulum_v0 import ReplayBuffer, DDPG\n",
    "from my_ddpg_ac import Actor, Critic\n",
    "import numpy as np\n",
    "import gym\n",
    "import pylab\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "\n",
    "from keras import layers, models, optimizers\n",
    "from keras import backend as K\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "\n",
    "import random\n",
    "from collections import namedtuple, deque \n",
    "\n",
    "!pip install gym keras==2.1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "gti2HevUH5tD",
    "outputId": "dfbf06f5-99ec-43a8-8d20-ecd321dd9ebc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!pip install gym pyvirtualdisplay > /dev/null 2>&1\\n!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\\n\\n#other helper functions\\nimport gym\\nfrom gym import logger as gymlogger\\nfrom gym.wrappers import Monitor\\ngymlogger.set_level(40) #error only\\nimport tensorflow as tf\\nimport numpy as np\\nimport random\\nimport matplotlib\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\nimport math\\nimport glob\\nimport io\\nimport base64\\nfrom IPython.display import HTML\\n\\nfrom IPython import display as ipythondisplay\\n\\n#others\\nfrom pyvirtualdisplay import Display\\ndisplay = Display(visible=0, size=(1400, 900))\\ndisplay.start()'"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rendering dependencies\n",
    "#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n",
    "\"\"\"!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
    "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
    "\n",
    "#other helper functions\n",
    "import gym\n",
    "from gym import logger as gymlogger\n",
    "from gym.wrappers import Monitor\n",
    "gymlogger.set_level(40) #error only\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "#others\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "cehzeJPIIhkN",
    "outputId": "a8a65aab-8d7f-448e-9caf-7c70281cbb99"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef show_video():\\n  mp4list = glob.glob(\\'video/*.mp4\\')\\n  if len(mp4list) > 0:\\n    mp4 = mp4list[0]\\n    video = io.open(mp4, \\'r+b\\').read()\\n    encoded = base64.b64encode(video)\\n    ipythondisplay.display(HTML(data=\\'\\'\\'<video alt=\"test\" autoplay \\n                loop controls style=\"height: 400px;\">\\n                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\\n             </video>\\'\\'\\'.format(encoded.decode(\\'ascii\\'))))\\n  else: \\n    print(\"Could not find video\")\\n    \\n\\ndef wrap_env(env):\\n  env = Monitor(env, \\'./video\\', force=True)\\n  return env\\n  '"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Utility functions to enable video recording of gym environment and displaying it\n",
    "To enable video, just do \"env = wrap_env(env)\"\"\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "def show_video():\n",
    "  mp4list = glob.glob('video/*.mp4')\n",
    "  if len(mp4list) > 0:\n",
    "    mp4 = mp4list[0]\n",
    "    video = io.open(mp4, 'r+b').read()\n",
    "    encoded = base64.b64encode(video)\n",
    "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "  else: \n",
    "    print(\"Could not find video\")\n",
    "    \n",
    "\n",
    "def wrap_env(env):\n",
    "  env = Monitor(env, './video', force=True)\n",
    "  return env\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3GoiXN8ZHOES"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"fixed-size buffer to store experience tuples\"\"\"\n",
    "\n",
    "    def __init__(self, buffer_size, batch_size):\n",
    "        \"\"\"initialize a replaybuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size:  max size of buffer\n",
    "            batch_size: size of each training batch\n",
    "        \"\"\"\n",
    "        self.memory = deque(maxlen=buffer_size) #100k\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        \n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self, batch_size=64):\n",
    "        \"Randomly return a batch of experience from memory\"\n",
    "        return random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory\"\"\"\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EQxYDmP1HOEV"
   },
   "outputs": [],
   "source": [
    "class DDPG():\n",
    "    \"\"\"Reinforcement learning agent that learns using DDPG\"\"\"\n",
    "    #change from action size to action range I think\n",
    "    def __init__(self, state_size, action_size, action_max):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size #1\n",
    "        self.action_range = action_max * 2\n",
    "        self.action_low = action_max - self.action_range\n",
    "        self.action_high = action_max\n",
    "        print(\"self.action_low\", self.action_low) #-2.\n",
    "        print(\"self.action_high\", self.action_high) #2.\n",
    "        #Actor Policy Model \n",
    "        self.actor_local = Actor(self.state_size, self.action_size, \\\n",
    "            self.action_low, self.action_high)\n",
    "        self.actor_target = Actor(self.state_size, self.action_size, \\\n",
    "            self.action_low, self.action_high)\n",
    "\n",
    "        #Critic Value Model\n",
    "        self.critic_local = Critic(self.state_size, self.action_size)\n",
    "        self.critic_target = Critic(self.state_size, self.action_size)\n",
    "\n",
    "        #initialize target model parameters with local model parameters\n",
    "        self.critic_target.model.set_weights(self.critic_local.model.get_weights())\n",
    "        self.actor_target.model.set_weights(self.actor_local.model.get_weights())\n",
    "\n",
    "        #Noise process\n",
    "        self.exploration_mu = 0\n",
    "        self.exploration_theta = 0.15\n",
    "        self.exploration_sigma = 0.2\n",
    "        #self.noise = OUNoise(self.action_size, self.exploration_mu, \\\n",
    "            #self.exploration_theta, self.exploration_sigma)\n",
    "        #set self.noise to whatever equation gives, not reset noise after episodes\n",
    "        #self.noise_decay = 0.99\n",
    "        #self.noise = 0\n",
    "\n",
    "        #replay memory\n",
    "        self.buffer_size = 100000\n",
    "        self.batch_size = 64\n",
    "        self.memory = ReplayBuffer(self.buffer_size, self.batch_size)\n",
    "\n",
    "        #Algorithm parameters\n",
    "        self.gamma = 0.99 #discount parameter for critic TD error\n",
    "        self.tau = 0.001 #soft update parameter\n",
    "        \"\"\"\n",
    "        self.tau_actor = 0.1\n",
    "        self.tau_critic = 0.5 \n",
    "        \"\"\"\n",
    "        self.render = True\n",
    "\n",
    "        self.load = False\n",
    "\n",
    "    #resets noise and env and outputs the initial state \n",
    "    def reset_episode(self):\n",
    "        #self.noise.reset()\n",
    "        state = env.reset()\n",
    "        self.last_state = state \n",
    "        return state\n",
    "\n",
    "    #adds the experience to memory and calls learn fxn if memory sufficient for batch\n",
    "    #\n",
    "    def step(self, action, reward, next_state, done):\n",
    "        self.memory.add(self.last_state, action, reward, next_state, \\\n",
    "            done)\n",
    "       \n",
    "        #Learn, if enough samples are available in memory\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences)\n",
    "        #Roll over last state and action\n",
    "        self.last_state = next_state\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Returns actions for given state(s) as per current policy\"\"\"\n",
    "        state = np.reshape(state, [-1, self.state_size])\n",
    "        raw_action = self.actor_local.model.predict(state)[0]\n",
    "        #noise = self.noise.sample()\n",
    "        #action = np.clip(raw_action + noise, -2, 2) #add noise for exploration\n",
    "        return raw_action \n",
    "\n",
    "    def learn(self, experiences):\n",
    "        \"\"\"update policy and value parameters using given batch of \n",
    "        experience tuples\"\"\"\n",
    "        #convert experience tuples to separate arrays for each element\n",
    "        #(states, actions, rewards, etc)\n",
    "        states = np.vstack([e.state for e in experiences if e is not None])\n",
    "        actions = np.array([e.action for e in experiences if e is not None]) \\\n",
    "            .astype(np.float32).reshape(-1, self.action_size)\n",
    "        rewards = np.array([e.reward for e in experiences if e is not None]) \\\n",
    "            .astype(np.float32).reshape(-1, 1)\n",
    "        dones = np.array([e.done for e in experiences if e is not None]) \\\n",
    "            .astype(np.uint8).reshape(-1, 1)\n",
    "        next_states = np.vstack([e.next_state for e in experiences if e is not None])               \n",
    "\n",
    "        #get predicted next_state actions and Q values from target models\n",
    "        actions_next = self.actor_target.model.predict_on_batch(next_states)\n",
    "        #use the actions from target actor to find target Qs of next actions\n",
    "        Q_targets_next = self.critic_target.model.predict_on_batch([next_states, actions_next])\n",
    "        #compute Q targets for current states using Q targets next from target_model\n",
    "        #and train local critic model\n",
    "        Q_targets = rewards + self.gamma * Q_targets_next * (1 - dones)\n",
    "        self.critic_local.model.train_on_batch(x=[states, actions], y=Q_targets)\n",
    "\n",
    "        \"\"\"called to get dQ(s,a)/da from critic\n",
    "        self.get_action_gradients = K.function(inputs=[*self.model.input, K.learning_phase()], outputs=action_gradients)\n",
    "        \"\"\"\n",
    "        action_gradients = np.reshape(self.critic_local.get_action_gradients([states, actions, 0]), \\\n",
    "            (-1, self.action_size))\n",
    "\n",
    "        \"\"\"use the dQ(s,a)/da from critic to train actor\n",
    "        called:  \n",
    "        self.train_fn = K.function(inputs=[self.model.input, action_gradients, K.learning_phase()], \\\n",
    "            outputs=[], updates=updates_op) no outputs\n",
    "        \"\"\"\n",
    "        self.actor_local.train_fn([states, action_gradients, 1])\n",
    "\n",
    "        #soft update target models, could use different tau for critic and actor\n",
    "        self.soft_update(self.critic_local.model, self.critic_target.model)\n",
    "        self.soft_update(self.actor_local.model, self.actor_target.model)\n",
    "\n",
    "        #soft update target models\n",
    "    def soft_update(self, local_model, target_model):\n",
    "        \"\"\"soft update model parameters\"\"\"\n",
    "        local_weights = np.array(local_model.get_weights())\n",
    "        target_weights = np.array(target_model.get_weights())\n",
    "\n",
    "        assert len(local_weights) == len(target_weights)\n",
    "        #weights to train target get updated using polyak formula where tau = [0,1]\n",
    "        #closer to 1 than 0 usually\n",
    "        new_weights = self.tau * local_weights + (1 - self.tau) * target_weights\n",
    "        target_model.set_weights(new_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hrl3roElHOEY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QAli6J_dHOEc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-GRWbUiMHOEf"
   },
   "outputs": [],
   "source": [
    "EPISODES = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3129
    },
    "colab_type": "code",
    "id": "PQ9-jRJjHOEi",
    "outputId": "623e376c-d7bb-4d66-e291-4bdc4ffd9501"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_size, action_size, action_bound 3 1 [2.]\n",
      "self.action_low [-2.]\n",
      "self.action_high [2.]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "states (InputLayer)          (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_49 (GaussianN (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_50 (GaussianN (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_51 (GaussianN (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "raw_actions (Dense)          (None, 1)                 33        \n",
      "_________________________________________________________________\n",
      "lambda_7 (Lambda)            (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 4,353\n",
      "Trainable params: 4,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/my_ddpg_ac.py:68: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"la..., inputs=Tensor(\"st...)`\n",
      "  self.model = models.Model(input=states, outputs=(raw_actions))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "states (InputLayer)          (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_52 (GaussianN (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_53 (GaussianN (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_54 (GaussianN (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "raw_actions (Dense)          (None, 1)                 33        \n",
      "_________________________________________________________________\n",
      "lambda_8 (Lambda)            (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 4,353\n",
      "Trainable params: 4,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "states (InputLayer)             (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "actions (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_49 (Dense)                (None, 32)           128         states[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_51 (Dense)                (None, 32)           64          actions[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 32)           0           dense_49[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 32)           0           dense_51[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise_55 (GaussianNois (None, 32)           0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise_57 (GaussianNois (None, 32)           0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_50 (Dense)                (None, 64)           2112        gaussian_noise_55[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_52 (Dense)                (None, 64)           2112        gaussian_noise_57[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 64)           0           dense_50[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 64)           0           dense_52[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise_56 (GaussianNois (None, 64)           0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise_58 (GaussianNois (None, 64)           0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 64)           0           gaussian_noise_56[0][0]          \n",
      "                                                                 gaussian_noise_58[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 64)           0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise_59 (GaussianNois (None, 64)           0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "q_values (Dense)                (None, 1)            65          gaussian_noise_59[0][0]          \n",
      "==================================================================================================\n",
      "Total params: 4,481\n",
      "Trainable params: 4,481\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Q_values Tensor(\"q_values_6/BiasAdd:0\", shape=(?, 1), dtype=float32) actions Tensor(\"actions_6:0\", shape=(?, 1), dtype=float32)\n",
      "action_gradients [<tf.Tensor 'gradients_14/dense_51/MatMul_grad/MatMul:0' shape=(?, 1) dtype=float32>]\n",
      "action_gradients type <class 'list'>\n",
      "self.get_action_gradients <keras.backend.tensorflow_backend.Function object at 0x7fe12d6d06d8>\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "states (InputLayer)             (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "actions (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_53 (Dense)                (None, 32)           128         states[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_55 (Dense)                (None, 32)           64          actions[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 32)           0           dense_53[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 32)           0           dense_55[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise_60 (GaussianNois (None, 32)           0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise_62 (GaussianNois (None, 32)           0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_54 (Dense)                (None, 64)           2112        gaussian_noise_60[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_56 (Dense)                (None, 64)           2112        gaussian_noise_62[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 64)           0           dense_54[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 64)           0           dense_56[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise_61 (GaussianNois (None, 64)           0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise_63 (GaussianNois (None, 64)           0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 64)           0           gaussian_noise_61[0][0]          \n",
      "                                                                 gaussian_noise_63[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 64)           0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise_64 (GaussianNois (None, 64)           0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "q_values (Dense)                (None, 1)            65          gaussian_noise_64[0][0]          \n",
      "==================================================================================================\n",
      "Total params: 4,481\n",
      "Trainable params: 4,481\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Q_values Tensor(\"q_values_7/BiasAdd:0\", shape=(?, 1), dtype=float32) actions Tensor(\"actions_7:0\", shape=(?, 1), dtype=float32)\n",
      "action_gradients [<tf.Tensor 'gradients_15/dense_55/MatMul_grad/MatMul:0' shape=(?, 1) dtype=float32>]\n",
      "action_gradients type <class 'list'>\n",
      "self.get_action_gradients <keras.backend.tensorflow_backend.Function object at 0x7fe123f1f908>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-a8c9ab20aaa4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;31m#reward += state[0] + 0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m#save experience in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;31m#bookkeeping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-d31ecf0fbc82>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;31m#Roll over last state and action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-d31ecf0fbc82>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, experiences)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action_gradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction_gradients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \"\"\"\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0maction_gradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_local\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \"\"\"use the dQ(s,a)/da from critic to train actor\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__== \"__main__\":\n",
    "\n",
    "    env=gym.make(\"Pendulum-v0\")\n",
    "\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.shape[0]\n",
    "    action_bound = env.action_space.high\n",
    "    print(\"state_size, action_size, action_bound\", state_size, action_size, action_bound)\n",
    "    #ensure action bound is symmetric\n",
    "    assert (env.action_space.high == -env.action_space.low)\n",
    "\n",
    "    agent = DDPG(state_size, action_size, action_bound)\n",
    "\n",
    "    scores, episodes, means, stds = [], [], [], []\n",
    "\n",
    "    position = 0\n",
    "\n",
    "    for episode in range(EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "        mean, std, large = 0, 0, 0\n",
    "        state = agent.reset_episode()\n",
    "\n",
    "        while not done:\n",
    "\n",
    "            #self.noise_decay *= \n",
    "            #get action from network\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            #adjust reward for incremental improvements in position\n",
    "            #special reward for mountaincarcontinuous\n",
    "            #state[0] is the cart position from -1.2 to 0.6\n",
    "            #reward += state[0] + 0.5\n",
    "            #save experience in memory\n",
    "            agent.step(action=action, reward=reward, next_state=next_state, done=done)\n",
    "            \n",
    "            #bookkeeping\n",
    "            score += reward\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "#retype from here\n",
    "                scores.append(score)\n",
    "                mean = np.mean(scores[-min(10, len(scores)):])\n",
    "                means.append(mean)\n",
    "                std = np.std(scores[-min(10, len(scores)):])\n",
    "                stds.append(std)\n",
    "                large = max(scores)\n",
    "                episodes.append(episode)\n",
    "                pylab.plot(episodes, scores, 'b')\n",
    "                pylab.plot(episodes, means, 'g')\n",
    "                pylab.plot(episodes, stds, 'r')\n",
    "                pylab.savefig(\"DDPG_pendulum_v0.png\")\n",
    "                print('episode:', episode, 'score:', score, 'best:', round(large,2), 'mean:', round(mean,2), 'std:', round(std, 2))\n",
    "                if np.mean(scores[-min(100, len(scores)):]) >= 90.0:\n",
    "                    sys.exit()\n",
    "                  \n",
    "        \n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G484_g5zHOEn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U_Pe388DHOEq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Pendulum_v0_ddpg_Colab.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
