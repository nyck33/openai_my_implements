{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pendulum-v0 DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from my_ddpg_pendulum_v0 import ReplayBuffer, DDPG\n",
    "from my_ddpg_ac import Actor, Critic, OUNoise\n",
    "import numpy as np\n",
    "import gym\n",
    "import pylab\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "\n",
    "from keras import layers, models, optimizers\n",
    "from keras import backend as K\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "\n",
    "import random\n",
    "from collections import namedtuple, deque \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"fixed-size buffer to store experience tuples\"\"\"\n",
    "\n",
    "    def __init__(self, buffer_size, batch_size):\n",
    "        \"\"\"initialize a replaybuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size:  max size of buffer\n",
    "            batch_size: size of each training batch\n",
    "        \"\"\"\n",
    "        self.memory = deque(maxlen=buffer_size) #100k\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        \n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self, batch_size=64):\n",
    "        \"Randomly return a batch of experience from memory\"\n",
    "        return random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory\"\"\"\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG():\n",
    "    \"\"\"Reinforcement learning agent that learns using DDPG\"\"\"\n",
    "    #change from action size to action range I think\n",
    "    def __init__(self, state_size, action_size, action_max):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size #1\n",
    "        self.action_range = action_max * 2\n",
    "        self.action_low = action_max - self.action_range\n",
    "        self.action_high = action_max\n",
    "        print(\"self.action_low\", self.action_low) #-2.\n",
    "        print(\"self.action_high\", self.action_high) #2.\n",
    "        #Actor Policy Model \n",
    "        self.actor_local = Actor(self.state_size, self.action_size, \\\n",
    "            self.action_low, self.action_high)\n",
    "        self.actor_target = Actor(self.state_size, self.action_size, \\\n",
    "            self.action_low, self.action_high)\n",
    "\n",
    "        #Critic Value Model\n",
    "        self.critic_local = Critic(self.state_size, self.action_size)\n",
    "        self.critic_target = Critic(self.state_size, self.action_size)\n",
    "\n",
    "        #initialize target model parameters with local model parameters\n",
    "        self.critic_target.model.set_weights(self.critic_local.model.get_weights())\n",
    "        self.actor_target.model.set_weights(self.actor_local.model.get_weights())\n",
    "\n",
    "        #Noise process\n",
    "        self.exploration_mu = 0\n",
    "        self.exploration_theta = 0.15\n",
    "        self.exploration_sigma = 0.2\n",
    "        self.noise = OUNoise(self.action_size, self.exploration_mu, \\\n",
    "            self.exploration_theta, self.exploration_sigma)\n",
    "\n",
    "        #replay memory\n",
    "        self.buffer_size = 100000\n",
    "        self.batch_size = 64\n",
    "        self.memory = ReplayBuffer(self.buffer_size, self.batch_size)\n",
    "\n",
    "        #Algorithm parameters\n",
    "        self.gamma = 0.99 #discount parameter for critic TD error\n",
    "        self.tau = 0.001 #soft update parameter\n",
    "        \"\"\"\n",
    "        self.tau_actor = 0.1\n",
    "        self.tau_critic = 0.5 \n",
    "        \"\"\"\n",
    "        self.render = True\n",
    "\n",
    "        self.load = False\n",
    "\n",
    "        if self.load:\n",
    "            self.actor.load(\"./saved_models/DDPG_pendulum_v0_actor.h5\")\n",
    "            self.critic.load(\"./saved_models/DDPG_pendulum_v0_critic.h5\")\n",
    "            \n",
    "    #resets noise and env and outputs the initial state \n",
    "    def reset_episode(self):\n",
    "        self.noise.reset()\n",
    "        state = env.reset()\n",
    "        self.last_state = state \n",
    "        return state\n",
    "\n",
    "    #adds the experience to memory and calls learn fxn if memory sufficient for batch\n",
    "    #\n",
    "    def step(self, action, reward, next_state, done):\n",
    "        self.memory.add(self.last_state, action, reward, next_state, \\\n",
    "            done)\n",
    "       \n",
    "        #Learn, if enough samples are available in memory\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences)\n",
    "        #Roll over last state and action\n",
    "        self.last_state = next_state\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Returns actions for given state(s) as per current policy\"\"\"\n",
    "        state = np.reshape(state, [-1, self.state_size])\n",
    "        raw_action = self.actor_local.model.predict(state)[0]\n",
    "        #rescale output to -2 to 2\n",
    "        scaled_action = raw_action * 2\n",
    "        noise = self.noise.sample()\n",
    "        action = np.clip((scaled_action + noise), -2, 2) #add noise for exploration\n",
    "        return action \n",
    "\n",
    "    def learn(self, experiences):\n",
    "        \"\"\"update policy and value parameters using given batch of \n",
    "        experience tuples\"\"\"\n",
    "        #convert experience tuples to separate arrays for each element\n",
    "        #(states, actions, rewards, etc)\n",
    "        states = np.vstack([e.state for e in experiences if e is not None])\n",
    "        actions = np.array([e.action for e in experiences if e is not None]) \\\n",
    "            .astype(np.float32).reshape(-1, self.action_size)\n",
    "        rewards = np.array([e.reward for e in experiences if e is not None]) \\\n",
    "            .astype(np.float32).reshape(-1, 1)\n",
    "        dones = np.array([e.done for e in experiences if e is not None]) \\\n",
    "            .astype(np.uint8).reshape(-1, 1)\n",
    "        next_states = np.vstack([e.next_state for e in experiences if e is not None])               \n",
    "\n",
    "        #get predicted next_state actions and Q values from target models\n",
    "        actions_next = self.actor_target.model.predict_on_batch(next_states)\n",
    "        #use the actions from target actor to find target Qs of next actions\n",
    "        Q_targets_next = self.critic_target.model.predict_on_batch([next_states, actions_next])\n",
    "        #compute Q targets for current states using Q targets next from target_model\n",
    "        #and train local critic model\n",
    "        Q_targets = rewards + self.gamma * Q_targets_next * (1 - dones)\n",
    "        self.critic_local.model.train_on_batch(x=[states, actions], y=Q_targets)\n",
    "\n",
    "        \"\"\"called to get dQ(s,a)/da from critic\n",
    "        self.get_action_gradients = K.function(inputs=[*self.model.input, K.learning_phase()], outputs=action_gradients)\n",
    "        \"\"\"\n",
    "        action_gradients = np.reshape(self.critic_local.get_action_gradients([states, actions, 0]), \\\n",
    "            (-1, self.action_size))\n",
    "\n",
    "        \"\"\"use the dQ(s,a)/da from critic to train actor\n",
    "        called:  \n",
    "        self.train_fn = K.function(inputs=[self.model.input, action_gradients, K.learning_phase()], \\\n",
    "            outputs=[], updates=updates_op) no outputs\n",
    "        \"\"\"\n",
    "        self.actor_local.train_fn([states, action_gradients, 1])\n",
    "\n",
    "        #soft update target models, could use different tau for critic and actor\n",
    "        self.soft_update(self.critic_local.model, self.critic_target.model)\n",
    "        self.soft_update(self.actor_local.model, self.actor_target.model)\n",
    "\n",
    "        #soft update target models\n",
    "    def soft_update(self, local_model, target_model):\n",
    "        \"\"\"soft update model parameters\"\"\"\n",
    "        local_weights = np.array(local_model.get_weights())\n",
    "        target_weights = np.array(target_model.get_weights())\n",
    "\n",
    "        assert len(local_weights) == len(target_weights)\n",
    "        #weights to train target get updated using polyak formula where tau = [0,1]\n",
    "        #closer to 1 than 0 usually\n",
    "        new_weights = self.tau * local_weights + (1 - self.tau) * target_weights\n",
    "        target_model.set_weights(new_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_size, action_size, action_bound 3 1 [2.]\n",
      "self.action_low [-2.]\n",
      "self.action_high [2.]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "states (InputLayer)          (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "raw_actions (Dense)          (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 4,353\n",
      "Trainable params: 4,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/nobu/Ubuntu Backup/_DeepLearning/_1openAI/_1gym/reinforcement-learning-copied/_1my_imps/continuous/Pendulum-v0/my_ddpg_ac.py:65: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"ra..., inputs=Tensor(\"st...)`\n",
      "  self.model = models.Model(input=states, outputs=(raw_actions))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "states (InputLayer)          (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "raw_actions (Dense)          (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 4,353\n",
      "Trainable params: 4,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "states (InputLayer)             (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "actions (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_35 (Dense)                (None, 32)           128         states[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_37 (Dense)                (None, 32)           64          actions[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 32)           0           dense_35[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 32)           0           dense_37[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_36 (Dense)                (None, 64)           2112        activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_38 (Dense)                (None, 64)           2112        activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 64)           0           dense_36[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 64)           0           dense_38[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 64)           0           activation_40[0][0]              \n",
      "                                                                 activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 64)           0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "q_values (Dense)                (None, 1)            65          activation_43[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 4,481\n",
      "Trainable params: 4,481\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Q_values Tensor(\"q_values_4/BiasAdd:0\", shape=(?, 1), dtype=float32) actions Tensor(\"actions_4:0\", shape=(?, 1), dtype=float32)\n",
      "action_gradients [<tf.Tensor 'gradients_10/dense_37/MatMul_grad/MatMul:0' shape=(?, 1) dtype=float32>]\n",
      "action_gradients type <class 'list'>\n",
      "self.get_action_gradients <keras.backend.tensorflow_backend.Function object at 0x7fe96bf00cc0>\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "states (InputLayer)             (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "actions (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_39 (Dense)                (None, 32)           128         states[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_41 (Dense)                (None, 32)           64          actions[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 32)           0           dense_39[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 32)           0           dense_41[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_40 (Dense)                (None, 64)           2112        activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_42 (Dense)                (None, 64)           2112        activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 64)           0           dense_40[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 64)           0           dense_42[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 64)           0           activation_45[0][0]              \n",
      "                                                                 activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 64)           0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "q_values (Dense)                (None, 1)            65          activation_48[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 4,481\n",
      "Trainable params: 4,481\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Q_values Tensor(\"q_values_5/BiasAdd:0\", shape=(?, 1), dtype=float32) actions Tensor(\"actions_5:0\", shape=(?, 1), dtype=float32)\n",
      "action_gradients [<tf.Tensor 'gradients_11/dense_41/MatMul_grad/MatMul:0' shape=(?, 1) dtype=float32>]\n",
      "action_gradients type <class 'list'>\n",
      "self.get_action_gradients <keras.backend.tensorflow_backend.Function object at 0x7fe96be31ef0>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/nobu/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "episode: 0 score: -1705.9136354877292 best: -1705.91 mean: -1705.91 std: 0.0\n",
      "episode: 1 score: -1294.1876698609503 best: -1294.19 mean: -1500.05 std: 205.86\n",
      "episode: 2 score: -1485.6185492002496 best: -1294.19 mean: -1495.24 std: 168.22\n",
      "episode: 3 score: -1055.9106003486338 best: -1055.91 mean: -1385.41 std: 239.61\n",
      "episode: 4 score: -1624.738930734448 best: -1055.91 mean: -1433.27 std: 234.73\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-80d8fd3eeab0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m#reward += state[0] + 0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;31m#save experience in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;31m#bookkeeping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-da5813f28256>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;31m#Roll over last state and action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-da5813f28256>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, experiences)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;31m#and train local critic model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mQ_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mQ_targets_next\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_local\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mQ_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \"\"\"called to get dQ(s,a)/da from critic\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1837\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1838\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1839\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1840\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__== \"__main__\":\n",
    "\n",
    "    env=gym.make(\"Pendulum-v0\")\n",
    "\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.shape[0]\n",
    "    action_bound = env.action_space.high\n",
    "    print(\"state_size, action_size, action_bound\", state_size, action_size, action_bound)\n",
    "    #ensure action bound is symmetric\n",
    "    assert (env.action_space.high == -env.action_space.low)\n",
    "\n",
    "    agent = DDPG(state_size, action_size, action_bound)\n",
    "\n",
    "    scores, episodes, means, stds = [], [], [], []\n",
    "\n",
    "    position = 0\n",
    "\n",
    "    for episode in range(EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "        mean, std, large = 0, 0, 0\n",
    "        state = agent.reset_episode()\n",
    "\n",
    "        while not done:\n",
    "            if agent.render:\n",
    "                env.render()\n",
    "\n",
    "            #get action from network\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \"\"\"adjust reward for incremental improvements in position\n",
    "            special reward for mountaincarcontinuous\n",
    "            state[0] is the cart position from -1.2 to 0.6\"\"\" \n",
    "            #reward += state[0] + 0.5\n",
    "            #save experience in memory\n",
    "            agent.step(action=action, reward=reward, next_state=next_state, done=done)\n",
    "            \n",
    "            #bookkeeping\n",
    "            score += reward\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "#retype from here\n",
    "                scores.append(score)\n",
    "                mean = np.mean(scores[-min(10, len(scores)):])\n",
    "                means.append(mean)\n",
    "                std = np.std(scores[-min(10, len(scores)):])\n",
    "                stds.append(std)\n",
    "                large = max(scores)\n",
    "                episodes.append(episode)\n",
    "                pylab.plot(episodes, scores, 'b')\n",
    "                pylab.plot(episodes, means, 'g')\n",
    "                pylab.plot(episodes, stds, 'r')\n",
    "                pylab.savefig(\"./saved_graphs/DDPG_pendulum_v0.png\")\n",
    "                print('episode:', episode, 'score:', score, 'best:', round(large,2), 'mean:', round(mean,2), 'std:', round(std, 2))\n",
    "                if np.mean(scores[-min(100, len(scores)):]) >= 90.0:\n",
    "                    sys.exit()\n",
    "\"\"\"                  \n",
    "        if episode % 50 == 0:\n",
    "            agent.actor_local.save_weights(\"./saved_models/DDPG_cartpoleV1_actor.h5\")\n",
    "            agent.critic_local.save_weights(\"./saved_models/DDPG_cartpoleV1_critic.h5\")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
